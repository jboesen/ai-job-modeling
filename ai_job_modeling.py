# -*- coding: utf-8 -*-
"""AI Job Modeling 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13UEEZS-kv3ZGP6QrZLCmazCmxV9CNJtS
"""

import numpy as np
import pandas as pd

import numpy as np, pandas as pd, matplotlib.pyplot as plt

rho, GAMMA, WAGE_FLOOR = 0.25, 1.2, 30
n_runs = 5
WAGE_ADJUSTMENT_RATE = 0.5
START, END, DT = 2025.0, 2032.0, 1/365
steps = int((END-START)/DT) + 1
time = np.linspace(START, END, steps)
MAX_AUTOMATION = 0.8
SECTORS = [
"DEV","FIN","BANK","GOV","CONS","INFRA","LAW","STARTUP",
"MANUF","HEALTH","EDUC","ENERGY","AGRO","LOGI","RETAIL","MEDIA",
"UTIL","WHOLE","TRANS","ACCOM","REAL","MINING","AI_GOV",
]
N = len(SECTORS)
idx = {s:i for i,s in enumerate(SECTORS)}

Pi = np.eye(N) * 0.70  # 70% of labour stays in own sector

# Finance
Pi[idx['FIN'], idx['LAW']]  = 0.12
Pi[idx['FIN'], idx['DEV']]  = 0.05
Pi[idx['FIN'], idx['CONS']] = 0.05
Pi[idx['FIN'], idx['FIN']]  = 0.70 - (0.12 + 0.05 + 0.05)

# Banking
Pi[idx['BANK'], idx['DEV']] = 0.08
Pi[idx['BANK'], idx['LAW']] = 0.08
Pi[idx['BANK'], idx['BANK']] = 0.70 - 0.16

# Tech
Pi[idx['DEV'], idx['FIN']]  = 0.10
Pi[idx['DEV'], idx['LAW']]  = 0.06
Pi[idx['DEV'], idx['DEV']]  = 0.70 - 0.16

# Health
Pi[idx['HEALTH'], idx['DEV']]   = 0.07
Pi[idx['HEALTH'], idx['CONS']]  = 0.06
Pi[idx['HEALTH'], idx['HEALTH']] = 0.70 - 0.13

# Retail
Pi[idx['RETAIL'], idx['LOGI']]  = 0.15
Pi[idx['RETAIL'], idx['FIN']]   = 0.05
Pi[idx['RETAIL'], idx['RETAIL']] = 0.70 - 0.20

# normalize rows
Pi = Pi / Pi.sum(axis=1, keepdims=True)

"""## Probabilistic"""

# task bundles and throughputs (length = 23 for the new AI_GOV sector)
THETA = {
    "startup":        np.array([455, 41, 25, 0, 364, 0, 33, 83,
                                0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0,
                                5], float),    # minimal AI_GOV involvement

    "ai_deploy":      np.array([420, 0, 0, 80, 50, 200, 0, 0,
                                0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0,
                                300], float),  # heavy governance review :contentReference[oaicite:0]{index=0}

    "loan":           np.array([0, 20, 45, 10, 0, 0, 15, 0,
                                0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0,
                                10], float),   # some oversight needed

    "audit":          np.array([0, 0, 0, 60, 0, 20, 40, 0,
                                0, 0, 0, 0, 0, 0, 0, 0,
                                0, 0, 0, 0, 0, 0,
                                20], float),   # governance checks :contentReference[oaicite:1]{index=1}

    "mfg_goods":      np.array([20, 0, 0, 5, 40, 50, 5, 0,
                                250, 0, 0, 120, 30, 80, 0, 10,
                                60, 40, 70, 0, 20, 30,
                                15], float),

    "ecommerce":      np.array([200, 30, 25, 5, 0, 40, 10, 0,
                                50, 0, 0, 20, 0, 150, 120, 60,
                                0, 30, 100, 0, 0, 0,
                                50], float),

    "hospital_care":  np.array([10, 20, 0, 15, 0, 40, 5, 0,
                                0, 300, 20, 50, 0, 40, 0, 0,
                                70, 0, 30, 80, 0, 0,
                                10], float),

    "school_service": np.array([25, 5, 0, 20, 0, 30, 5, 0,
                                0, 10, 300, 40, 0, 20, 0, 15,
                                30, 0, 30, 40, 0, 0,
                                5], float),

    "infra_build":    np.array([15, 10, 5, 25, 250, 120, 10, 0,
                                100, 0, 0, 200, 0, 80, 0, 5,
                                120, 40, 100, 0, 60, 150,
                                20], float),
}

KAPPA = {
    "startup":        0.80,
    "ai_deploy":      0.90,
    "loan":           0.95,
    "audit":          1.00,
    "mfg_goods":      0.90,
    "ecommerce":      0.85,
    "hospital_care":  0.95,
    "school_service": 1.00,
    "infra_build":    0.80,
    # AI_GOV efficiency: assumes specialized governance roles process at 90% of peak
    "ai_gov":         0.90,  # aligned with Chief AI Officer role growth :contentReference[oaicite:2]{index=2}
}

# Realistic mobility matrix based on BLS JOLTS data and academic research
# Annual mobility rates derived from churn rates and skill transferability studies

# Define skill clusters for similar mobility patterns
TECH_CLUSTER = ["DEV", "STARTUP", "MEDIA"]
FINANCE_CLUSTER = ["FIN", "BANK", "REAL"]
SERVICE_CLUSTER = ["RETAIL", "ACCOM", "WHOLE"]
PHYSICAL_CLUSTER = ["MANUF", "LOGI", "MINING", "AGRO"]
PROFESSIONAL_CLUSTER = ["LAW", "GOV", "AI_GOV"]
CARE_CLUSTER = ["HEALTH", "EDUC"]
INFRASTRUCTURE_CLUSTER = ["INFRA", "UTIL", "TRANS", "ENERGY"]
CONSULTING_CLUSTER = ["CONS"]

# Initialize mobility matrix with low baseline
M = np.full((N, N), 0.002)  # Very low cross-sector mobility as baseline
np.fill_diagonal(M, 0)

# High within-cluster mobility (based on skill similarity)
# Tech cluster - high internal mobility (annual rates)
for i in TECH_CLUSTER:
    for j in TECH_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.085  # 8.5% annual rate

# Finance cluster - moderate internal mobility (licensing barriers)
for i in FINANCE_CLUSTER:
    for j in FINANCE_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.045  # 4.5% annual rate

# Service cluster - highest mobility (low barriers)
for i in SERVICE_CLUSTER:
    for j in SERVICE_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.120  # 12% annual rate based on BLS data

# Physical/operations cluster - moderate mobility
for i in PHYSICAL_CLUSTER:
    for j in PHYSICAL_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.065  # 6.5% annual rate

# Professional cluster - low mobility (licensing/education barriers)
for i in PROFESSIONAL_CLUSTER:
    for j in PROFESSIONAL_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.025  # 2.5% annual rate

# Care cluster - low mobility (specialized training)
M[idx["HEALTH"], idx["EDUC"]] = 0.035
M[idx["EDUC"], idx["HEALTH"]] = 0.020  # Harder to move to health

# Infrastructure cluster - moderate mobility
for i in INFRASTRUCTURE_CLUSTER:
    for j in INFRASTRUCTURE_CLUSTER:
        if i != j:
            M[idx[i], idx[j]] = 0.055

# Cross-cluster mobility (adjacent skills)
# Tech to consulting (common path)
M[idx["DEV"], idx["CONS"]] = 0.040
M[idx["DEV"], idx["STARTUP"]] = 0.2
M[idx["STARTUP"], idx["DEV"]] = 0.2
M[idx["STARTUP"], idx["CONS"]] = 0.065

# Finance to consulting
M[idx["FIN"], idx["CONS"]] = 0.055
M[idx["BANK"], idx["CONS"]] = 0.045

# Consulting to other sectors (easier outflow)
M[idx["CONS"], idx["FIN"]] = 0.070
M[idx["CONS"], idx["BANK"]] = 0.050
M[idx["CONS"], idx["DEV"]] = 0.035
M[idx["CONS"], idx["STARTUP"]] = 0.080

# Service to other sectors (easy exit, hard entry)
M[idx["RETAIL"], idx["ACCOM"]] = 0.150  # Very high, same cluster
M[idx["ACCOM"], idx["RETAIL"]] = 0.140
M[idx["RETAIL"], idx["WHOLE"]] = 0.095
M[idx["WHOLE"], idx["RETAIL"]] = 0.110

# From service to other clusters (downward mobility common)
M[idx["RETAIL"], idx["MANUF"]] = 0.025
M[idx["RETAIL"], idx["LOGI"]] = 0.035
M[idx["ACCOM"], idx["RETAIL"]] = 0.140

# AI Governance - specialized mobility patterns
M[idx["DEV"], idx["AI_GOV"]] = 0.055      # Tech background helps
M[idx["GOV"], idx["AI_GOV"]] = 0.070      # Government experience valuable
M[idx["LAW"], idx["AI_GOV"]] = 0.045      # Legal background useful
M[idx["CONS"], idx["AI_GOV"]] = 0.035     # Consulting transferable

# From AI_GOV (new field, people may leave)
M[idx["AI_GOV"], idx["DEV"]] = 0.080      # Return to tech
M[idx["AI_GOV"], idx["GOV"]] = 0.060      # Move to traditional gov
M[idx["AI_GOV"], idx["LAW"]] = 0.040      # Move to law
M[idx["AI_GOV"], idx["CONS"]] = 0.090     # High consulting demand

# Manufacturing/Infrastructure connections
M[idx["MANUF"], idx["INFRA"]] = 0.045
M[idx["INFRA"], idx["MANUF"]] = 0.040
M[idx["ENERGY"], idx["UTIL"]] = 0.080
M[idx["UTIL"], idx["ENERGY"]] = 0.075

# Cross-industry moves that require minimal retraining
M[idx["LOGI"], idx["TRANS"]] = 0.095
M[idx["TRANS"], idx["LOGI"]] = 0.085
M[idx["WHOLE"], idx["LOGI"]] = 0.055
M[idx["LOGI"], idx["WHOLE"]] = 0.065

# Government sector (low mobility due to job security)
M[idx["GOV"], idx["LAW"]] = 0.035      # Gov to legal
M[idx["LAW"], idx["GOV"]] = 0.055      # Legal to gov (common)

# Media connections
M[idx["MEDIA"], idx["DEV"]] = 0.070    # Digital media to tech
M[idx["DEV"], idx["MEDIA"]] = 0.050    # Tech to media

# Mining/Energy (resource sectors)
M[idx["MINING"], idx["ENERGY"]] = 0.085
M[idx["ENERGY"], idx["MINING"]] = 0.075

# Professional services outflow to business
M[idx["LAW"], idx["FIN"]] = 0.030
M[idx["LAW"], idx["REAL"]] = 0.040

# Healthcare mobility (very restricted)
M[idx["HEALTH"], idx["EDUC"]] = 0.035   # Some transferability
M[idx["EDUC"], idx["HEALTH"]] = 0.015   # Very difficult

# Real estate connections
M[idx["REAL"], idx["FIN"]] = 0.055
M[idx["FIN"], idx["REAL"]] = 0.040
M[idx["REAL"], idx["CONS"]] = 0.045

# Banking specific flows
M[idx["BANK"], idx["FIN"]] = 0.080      # Banking to broader finance
M[idx["FIN"], idx["BANK"]] = 0.060      # Finance to banking

# Sector-specific flow thresholds based on barriers to entry
FLOW_THRESHOLD_MAP = {
    # Within high-mobility clusters (service jobs)
    ("RETAIL", "ACCOM"): 1.08,
    ("ACCOM", "WHOLE"): 1.10,
    ("RETAIL", "WHOLE"): 1.12,

    # Within tech cluster (skill-based)
    ("DEV", "STARTUP"): 1.25,
    ("STARTUP", "DEV"): 1.15,    # Easier to return to employment
    ("DEV", "MEDIA"): 1.30,
    ("MEDIA", "DEV"): 1.20,

    # Within finance cluster (licensing barriers)
    ("FIN", "BANK"): 1.35,
    ("BANK", "FIN"): 1.25,
    ("FIN", "REAL"): 1.40,
    ("REAL", "FIN"): 1.45,

    # Professional switches (high barriers)
    ("LAW", "GOV"): 1.45,
    ("GOV", "LAW"): 1.65,       # Harder to enter private practice
    ("HEALTH", "EDUC"): 1.50,
    ("EDUC", "HEALTH"): 2.80,   # Very difficult (licensing)

    # AI Governance (new field, varying barriers)
    ("DEV", "AI_GOV"): 1.35,
    ("GOV", "AI_GOV"): 1.25,    # Government experience valued
    ("LAW", "AI_GOV"): 1.50,
    ("AI_GOV", "DEV"): 1.15,    # Easy return to tech
    ("AI_GOV", "GOV"): 1.30,
    ("AI_GOV", "CONS"): 1.20,   # High demand for AI governance consultants

    # Cross-cluster moves (major career changes)
    ("RETAIL", "BANK"): 2.50,
    ("ACCOM", "FIN"): 2.80,
    ("MANUF", "HEALTH"): 3.20,
    ("MINING", "EDUC"): 3.50,
    ("LOGI", "LAW"): 3.00,

    # Infrastructure/operations
    ("MANUF", "INFRA"): 1.40,
    ("INFRA", "MANUF"): 1.35,
    ("ENERGY", "UTIL"): 1.25,
    ("UTIL", "ENERGY"): 1.30,

    # Consulting flows (generally easier due to transferable skills)
    ("CONS", "FIN"): 1.30,
    ("CONS", "BANK"): 1.40,
    ("CONS", "DEV"): 1.55,
    ("DEV", "CONS"): 1.25,
    ("FIN", "CONS"): 1.20,
    ("BANK", "CONS"): 1.25,
}

# Default threshold for undefined pairs
DEFAULT_FLOW_THRESHOLD = 1.00

def get_flow_threshold(from_sector, to_sector):
    """Get flow threshold for sector transition, with defaults for undefined pairs"""
    pair = (from_sector, to_sector)
    return FLOW_THRESHOLD_MAP[pair] if pair in FLOW_THRESHOLD_MAP else DEFAULT_FLOW_THRESHOLD

initial_L = np.array([
    19.11,  # DEV
    28.31,  # FIN
    19.55,  # BANK
   171.98,  # GOV
    13.96,  # CONS
    60.53,  # INFRA
     8.96,  # LAW
     2.26,  # STARTUP
    97.68,  # MANUF
   162.49,  # HEALTH
    28.82,  # EDUC
     4.49,  # ENERGY
    11.04,  # AGRO
    24.78,  # LOGI
   117.69,  # RETAIL
     4.89,  # MEDIA
     4.34,  # UTIL
    46.17,  # WHOLE
    24.78,  # TRANS
   125.26,  # ACCOM
    18.03,  # REAL
     4.49,  # MINING
     0.38,  # AI_GOV
], dtype=float)

# production
def automation_curve(t: float, mids: np.ndarray, slopes: np.ndarray) -> np.ndarray:
    z = np.clip(slopes * (t - mids), -700, 700)
    return 1.0 / (1.0 + np.exp(-z))

def cobb_douglas_output(lab_eff, theta, k_mult):
    s = theta / theta.sum()
    return k_mult * np.exp((s * np.log(lab_eff / theta)).sum())

def calculate_unemployment_rate(L, U):
    """Calculate unemployment rate over time"""
    total_labor_force = L.sum(axis=1) + U
    unemployment_rate = U / total_labor_force
    return unemployment_rate

BETA_TIGHTNESS = 0.4      # wage elasticity to tightness
GAMMA_PROD      = 1.0      # pass-through of productivity growth
MIN_U           = 1e-9     # avoid divide-by-zero

base_params = {
"DEV":(2026.5,2.0),"FIN":(2027.2,1.3),"BANK":(2027.4,1.1),
"GOV":(2027.0,1.0),"CONS":(2027.0,1.2),"INFRA":(2026.8,1.3),
"LAW":(2027.5,0.9),"STARTUP":(2026.6,1.4),"MANUF":(2026.3,2.1),
"HEALTH":(2028.0,1.0),"EDUC":(2028.4,0.8),"ENERGY":(2027.3,1.2),
"AGRO":(2027.2,1.1),"LOGI":(2026.5,1.6),"RETAIL":(2026.8,1.4),
"MEDIA":(2026.0,1.8),"UTIL":(2027.0,1.5),"WHOLE":(2028.2,1.1),
"TRANS":(2027.5,1.3),"ACCOM":(2028.0,0.9),"REAL":(2029.0,0.8),
"MINING":(2026.8,1.7),"AI_GOV":(2028.5,1.1),
}

mid_f = np.array([base_params[s][0] for s in SECTORS])
slope_f = np.array([base_params[s][1] for s in SECTORS])

base_phi = np.array([
4.0,3.0,2.5,1.8,2.2,2.0,1.4,1.0,
3.5,2.1,1.6,2.3,2.5,2.8,2.2,2.9,
80/40,70/40,75/40,65/40,90/40,95/40,1.8,
],dtype=float)
phi_f = base_phi.copy() # leverage on identical node set


# marginal-utility curvature by sector (β < 1  → stronger diminishing returns)
MU_BETA = np.array([
    0.91,  # DEV    – digital products scale well; utility declines slowly
    0.87,  # FIN    – high pay-off dispersion, faster saturation
    0.87,  # BANK   – similar to FIN
    0.95,  # GOV    – public-service provision shows ≈ constant returns
    0.86,  # CONS   – project work repeats skills
    0.86,  # INFRA  – large fixed-cost assets, moderate crowd-out
    0.93,  # LAW    – each new case retains high marginal value
    0.78,  # STARTUP– power-law outcomes; extra bets lose utility quickly
    0.85,  # MANUF  – evidence of slight decreasing returns
    0.94,  # HEALTH – additional care remains valuable
    0.94,  # EDUC   – meta-analysis finds (weakly) increasing returns
    0.84,  # ENERGY – capacity-driven saturation
    0.83,  # AGRO   – land & yield limits
    0.83,  # LOGI   – network gets congested
    0.80,  # RETAIL – quick saturation in local markets
    0.82,  # MEDIA  – attention saturates but not as fast as retail
    0.93,  # UTIL   – near-monopoly scale, marginal value still high
    0.82,  # WHOLE  – volume business, diminishing after bulk levels
    0.83,  # TRANS  – physical capacity limits
    0.79,  # ACCOM  – room-night inventory saturates quickly
    0.85,  # REAL   – land development shows mild decreasing returns
    0.82,  # MINING – ore grades fall with additional projects
    0.92,  # AI_GOV – specialised oversight; few substitutes so utility stays high
], dtype=float)

# initial wages
w0 = np.array([
     140.0, 130.0, 110.0,  90.0, 105.0, 100.0,  95.0,  70.0,
     105.0, 115.0,  80.0, 110.0,  70.0,  90.0,  75.0,  85.0,
      80.0,  70.0,  75.0,  65.0,  90.0,  95.0,  130.0,
])

def sample_params(rng):
    mids, slopes = zip(*base_params.values())
    mid_draw = rng.normal(mids, 0.4)
    slope_draw = rng.normal(slopes, 0.25).clip(min=0.3)
    phi_draw = rng.lognormal(np.log(base_phi), 0.15)

    growth_rate = rng.beta(8, 792)
    retire_rate = rng.beta(5, 829)
    demand_growth = rng.normal(0.012, 0.003)

    hire_rate = rng.beta(4, 16)
    fire_rate = rng.beta(10, 10)
    invest_rate = rng.beta(2, 8) * 0.2
    match_fx = rng.beta(3, 7) * 0.3
    mu_beta_draw = np.clip(rng.normal(MU_BETA, 0.05), 0.6, 1.0)

    # sampled task reallocation rate: mean ~0.15, bounded between [0.05, 0.3]
    reallocation_rate = np.clip(rng.normal(0.15, 0.04), 0.05, 0.3)

    return {
        "mids": mid_draw,
        "slopes": slope_draw,
        "phi": phi_draw,
        "growth": growth_rate,
        "retire": retire_rate,
        "demand_g": demand_growth,
        "hire": hire_rate,
        "fire": fire_rate,
        "invest": invest_rate,
        "match_fx": match_fx,
        "mu_beta": mu_beta_draw,
        "reallocation": reallocation_rate,
    }

def automation_curve(t: float, mids: np.ndarray, slopes: np.ndarray) -> np.ndarray:
    """Automation readiness: A_f(t) = 1/(1 + exp(-s_f(t - m_f)))"""
    z = np.clip(slopes * (t - mids), -700, 700)
    return 1.0 / (1.0 + np.exp(-z))

def get_flow_threshold(from_sector: str, to_sector: str) -> float:
    """Get flow threshold for sector transition"""
    pair = (from_sector, to_sector)
    return FLOW_THRESHOLD_MAP.get(pair, DEFAULT_FLOW_THRESHOLD)

def marginal_utility(opportunities: np.ndarray,
                    beta: np.ndarray,
                    rng: np.random.Generator,
                    lam: float = 1.0) -> np.ndarray:
    """Compute marginal utility with diminishing returns and startup effects"""
    opp = np.nan_to_num(opportunities, nan=0.0, posinf=0.0, neginf=0.0)
    opp = np.maximum(opp, 0.0)

    base_utility = np.power(1.0 + opp, beta) / (1.0 + opp)

    # Heavy-tailed startup modifier
    startup_idx = idx["STARTUP"]
    n_startups = int(opp[startup_idx])
    if n_startups > 0:
        startup_draws = rng.exponential(scale=1.0/lam, size=n_startups)
        startup_multiplier = min(startup_draws.mean(), 1.0)
        base_utility[startup_idx] *= startup_multiplier

    return base_utility

# dynamic task reassignment across job functions
TASK_REALLOCATION_RATE = 0.15   # annual share of excess labour that can retrain

def run_one_draw(seed: int):
    """dual-layer labour market simulation with task reassignment"""
    rng = np.random.default_rng(seed)
    p = sample_params(rng)

    L_sector = np.zeros((steps, N))
    L_function = np.zeros((steps, N))
    L_eff_function = np.zeros((steps, N))
    L_eff_sector = np.zeros((steps, N))
    A_level = np.zeros((steps, N))
    w = np.zeros((steps, N))
    U = np.zeros(steps)
    Y = {act: np.zeros(steps) for act in THETA}

    L_sector[0] = initial_L.copy()
    w[0] = w0.copy()
    U[0] = 5.0

    Pi_norm = Pi / Pi.sum(axis=1, keepdims=True)
    L_func_state = Pi_norm.T @ L_sector[0]      # mutable function-level workforce

    for k in range(steps - 1):
        L_function[k] = L_func_state

        # automation
        A_level[k] = automation_curve(time[k], p["mids"], p["slopes"]).clip(0.0, MAX_AUTOMATION)
        L_eff_function[k] = (1.0 + p["phi"] * A_level[k]) * L_func_state
        func_tot = Pi_norm.sum(axis=0).clip(min=1e-9)
        L_eff_sector[k] = Pi_norm @ (L_eff_function[k] / func_tot)

        # population
        total_pop = L_sector[k].sum() + U[k]
        pop_growth = total_pop * (p["growth"] - p["retire"]) * DT

        # demand
        D_task = np.zeros(N)
        for activity, theta in THETA.items():
            m = theta > 0
            if m.any() and L_eff_sector[k, m].sum() > 1e-9:
                cap_ratio = L_eff_sector[k, m] / theta[m]
                prod = KAPPA[activity] * cap_ratio.min()
                auto_fac = (1.0 + p["phi"][m] * A_level[k, m]).clip(min=1e-6)
                cost_red = auto_fac ** 0.2
                D_task[m] += theta[m] * prod / cost_red
                Y[activity][k] = prod

        D_base = L_sector[k] * (1.0 + p["demand_g"] * DT)
        D_total = 0.6 * D_base + 0.4 * D_task
        D_adjusted = D_total * marginal_utility(D_total, p["mu_beta"], rng)

        # wages
        tight = D_adjusted / np.maximum(L_eff_sector[k], 1e-6)
        w_new = w[k] * (1.0 + WAGE_ADJUSTMENT_RATE * DT * (tight - 1.0))
        w[k + 1] = np.maximum(WAGE_FLOOR, 0.5 * w[k] + 0.5 * w_new)

        # mobility
        gross = np.zeros((N, N))
        for f in range(N):
            for t in range(N):
                if f != t and M[f, t] > 0:
                    wd = w[k + 1, t] - w[k + 1, f]
                    if wd > 0 and tight[t] >= get_flow_threshold(SECTORS[f], SECTORS[t]):
                        gross[f, t] = M[f, t] * wd * L_sector[k, f] * DT
        net = gross.sum(axis=0) - gross.sum(axis=1)
        L_post_move = np.maximum(L_sector[k] + net, 0.0)

        # firing & hiring
        excess = np.maximum(L_post_move - D_adjusted, 0.0)
        fired = p["fire"] * DT * excess
        L_after_fire = L_post_move - fired
        U_after_fire = U[k] + fired.sum()
        unmet = np.maximum(D_adjusted - L_after_fire, 0.0)
        if unmet.sum() > 1e-9 and U_after_fire > 1e-9:
            hires = min(U_after_fire, p["hire"] * DT * unmet.sum()) * unmet / unmet.sum()
        else:
            hires = np.zeros(N)
        L_sector[k + 1] = np.maximum(L_after_fire + hires, 0.0)
        U[k + 1] = max(U_after_fire - hires.sum() + pop_growth, MIN_U)

        # task reassignment
        D_func = Pi_norm.T @ D_adjusted
        excess_f = np.maximum(L_func_state - D_func, 0.0)
        outflow = p["reallocation"] * DT * excess_f
        avail = outflow.sum()
        unmet_f = np.maximum(D_func - L_func_state, 0.0)
        inflow = avail * unmet_f / unmet_f.sum() if unmet_f.sum() > 0 else np.zeros_like(L_func_state)
        L_func_state = np.maximum(L_func_state - outflow + inflow, 0.0)

    return L_sector, w, U, Y, A_level

# run ensemble
all_L = np.zeros((n_runs, steps, N))
all_U = np.zeros((n_runs, steps))
all_w = np.zeros((n_runs, steps, N))
all_Y = {}
all_A = np.zeros((n_runs, steps, N))

for run in range(n_runs):
    L_sim, w_sim, U_sim, Y_sim, A_sim = run_one_draw(run)
    all_L[run], all_w[run], all_U[run] = L_sim, w_sim, U_sim
    all_Y[run] = Y_sim
    all_A[run] = A_sim
# summarise outcomes
median_L = np.median(all_L, axis=0)
p05_L = np.percentile(all_L, 5, axis=0)
p95_L = np.percentile(all_L, 95, axis=0)

import matplotlib.pyplot as plt
import numpy as np

# Calculate total labor force (employed + unemployed) for each run and time step
total_labor_force = all_L.sum(axis=2) + all_U  # sum across sectors + unemployed

# Calculate unemployment rate as percentage
unemployment_rate = np.divide(
    all_U,
    np.clip(total_labor_force, 1e-6, None),   # avoid 0
    out=np.zeros_like(all_U),
    where=total_labor_force > 0
) * 100

# Calculate unemployment rate statistics across all runs
median_rate = np.median(unemployment_rate, axis=0)
p05_rate = np.percentile(unemployment_rate, 5, axis=0)
p95_rate = np.percentile(unemployment_rate, 95, axis=0)

# Create the plot
plt.figure(figsize=(12, 8))
plt.fill_between(time, p05_rate, p95_rate, alpha=0.3, color='lightblue', label='90% Confidence Interval')
plt.plot(time, median_rate, color='darkblue', linewidth=2, label='Median Unemployment Rate')

plt.xlabel('Year')
plt.ylabel('Unemployment Rate (%)')
plt.title('Unemployment Rate Projections Over Time')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Print summary statistics
print(f"Initial unemployment rate: {median_rate[0]:.2f}%")
print(f"Final unemployment rate (median): {median_rate[-1]:.2f}%")
print(f"Peak unemployment rate: {median_rate.max():.2f}% in year {time[np.argmax(median_rate)]:.1f}")
print(f"90% CI at end: [{p05_rate[-1]:.2f}%, {p95_rate[-1]:.2f}%]")

years_to_check = [2025, 2030.0, 2032.0]
year_indices = {year: int((year - START) / DT) for year in years_to_check}

for year in years_to_check:
    idx_year = year_indices[year]
    salaries = median_L[idx_year]
    wages = np.median(all_w[:, idx_year], axis=0)
    data = list(zip(SECTORS, wages, salaries))
    sorted_data = sorted(data, key=lambda x: -x[1])

    print(f"\nYear {year:.0f} — sorted by descending salary:")
    print(f"{'Sector':<10} {'Salary':>10} {'Workforce':>12}")
    for sector, wage, labor in sorted_data:
        print(f"{sector:<10} {wage:10.2f} {labor:12.1f}")

"""# GDP"""

def calculate_gdp(L, w, time, Y, automation_data=None):
    """
    Calculate GDP properly accounting for automation productivity gains

    Parameters:
    L: array of shape (steps, N) - labor allocation over time
    w: array of shape (steps, N) - wages over time
    time: array of time values
    Y: dict of production outputs by activity
    automation_data: dict with automation factors A over time (if available)
    """

    # Value added per unit of actual output (these are the real GDP contributors)
    VALUE_ADDED_PER_UNIT = {
        "startup": 250.0,        # Value per startup created
        "ai_deploy": 180.0,      # Value per AI deployment
        "loan": 15.0,            # Value per loan processed
        "audit": 45.0,           # Value per audit completed
        "mfg_goods": 35.0,       # Value per manufactured good
        "ecommerce": 25.0,       # Value per ecommerce transaction
        "hospital_care": 120.0,  # Value per patient treated
        "school_service": 80.0,  # Value per student educated
        "infra_build": 200.0,    # Value per infrastructure project
    }

    # Base sector productivity (value added per worker-year, in thousands)
    BASE_PRODUCTIVITY = {
        "DEV": 280,      "FIN": 210,      "BANK": 190,     "GOV": 140,
        "CONS": 250,     "INFRA": 220,    "LAW": 230,      "STARTUP": 320,
        "MANUF": 240,    "HEALTH": 180,   "EDUC": 150,     "ENERGY": 260,
        "AGRO": 200,     "LOGI": 190,     "RETAIL": 160,   "MEDIA": 220,
        "UTIL": 200,     "WHOLE": 180,    "TRANS": 190,    "ACCOM": 140,
        "REAL": 210,     "MINING": 230,   "AI_GOV": 200,
    }

    productivity_array = np.array([BASE_PRODUCTIVITY[sector] for sector in SECTORS])

    steps, N = L.shape

    # Method 1: Activity-based GDP (most accurate)
    activity_gdp = np.zeros(steps)
    for t in range(steps):
        for activity, output in Y.items():
            if activity in VALUE_ADDED_PER_UNIT:
                # This captures the actual economic output regardless of automation level
                activity_gdp[t] += output[t] * VALUE_ADDED_PER_UNIT[activity]

    # Method 2: Sector-based GDP with automation adjustment
    sector_gdp = np.zeros(steps)

    if automation_data is not None:
        # If we have automation data, adjust productivity accordingly
        for t in range(steps):
            A = automation_data(t)  # Automation factor by sector
            # Productivity boost = 1 + phi * A (same as in your model's capacity calculation)
            productivity_boost = 1 + base_phi * A
            adjusted_productivity = productivity_array * productivity_boost
            sector_gdp[t] = (L[t] * adjusted_productivity * 1000).sum()

    # Method 3: Wage-bill based (least accurate but useful for comparison)
    wage_gdp = np.zeros(steps)
    for t in range(steps):
        total_wages = (L[t] * w[t] * 1000).sum()
        # Assume total compensation is about 60% of GDP (roughly accurate for US)
        wage_gdp[t] = total_wages / 0.6

    # Aggregate to yearly data
    years = np.floor(time).astype(int)
    unique_years = np.unique(years)

    gdp_data = []

    for year in unique_years:
        year_mask = years == year
        if not np.any(year_mask):
            continue

        # Average over the year
        yearly_activity_gdp = activity_gdp[year_mask].sum() * DT
        yearly_sector_gdp = sector_gdp[year_mask].sum() * DT
        yearly_wage_gdp = wage_gdp[year_mask].sum() * DT

        # Use activity-based as primary, sector-based as secondary
        primary_gdp = yearly_sector_gdp

        gdp_data.append({
            'year': year,
            'gdp_primary': primary_gdp / 1e9,  # Convert to billions
            'gdp_activity_based': yearly_activity_gdp / 1e9,
            'gdp_sector_based': yearly_sector_gdp / 1e9,
            'gdp_wage_based': yearly_wage_gdp / 1e9,
            'total_employment': L[year_mask].sum(axis=1).sum() * DT,
            'avg_wage': (w[year_mask] * L[year_mask]).sum() / L[year_mask].sum(),
        })

    return pd.DataFrame(gdp_data)

gdp_ensemble = []
for run in range(n_runs):
    gdp_run = calculate_gdp(
        all_L[run],
        all_w[run],
        time,
        all_Y[run],
        automation_data=lambda t, A_run=all_A[run]: A_run[t],
    )
    gdp_ensemble.append(gdp_run)

# Calculate median and confidence intervals
gdp_median = pd.concat(gdp_ensemble).groupby('year').median()
gdp_combined = pd.concat(gdp_ensemble)
gdp_median = gdp_combined.groupby('year').median()
gdp_p05 = gdp_combined.groupby('year').quantile(0.05)
gdp_p25 = gdp_combined.groupby('year').quantile(0.25)
gdp_p75 = gdp_combined.groupby('year').quantile(0.75)
gdp_p95 = gdp_combined.groupby('year').quantile(0.95)

"""## GDP Plotting"""

import numpy as np
import plotly.graph_objects as go
import pandas as pd

def hex_to_rgba(hex_color: str, alpha: float) -> str:
    hex_color = hex_color.lstrip("#")
    r, g, b = (int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    return f"rgba({r},{g},{b},{alpha})"

# # Create the GDP ensemble and calculate percentiles (from your code above)
# gdp_ensemble = []
# for run in range(n_runs):
#     gdp_run = calculate_gdp(all_L[run], all_w[run], time, all_Y[run])
#     gdp_ensemble.append(gdp_run)


# Extract years and GDP values
years = gdp_median.index.values
gdp_med = gdp_median['gdp_sector_based'].values
gdp_q05 = gdp_p05['gdp_sector_based'].values
gdp_q25 = gdp_p25['gdp_sector_based'].values
gdp_q75 = gdp_p75['gdp_sector_based'].values
gdp_q95 = gdp_p95['gdp_sector_based'].values

# Create the plot
fig = go.Figure()

# Color for GDP line
gdp_color = "#2E86AB"  # Nice blue color
band1 = hex_to_rgba(gdp_color, 0.20)   # 5–95% band
band2 = hex_to_rgba(gdp_color, 0.40)   # 25–75% band

# 5–95% confidence band
fig.add_trace(go.Scatter(
    x=np.concatenate([years, years[::-1]]),
    y=np.concatenate([gdp_q95, gdp_q05[::-1]]),
    fill="toself", fillcolor=band1,
    line=dict(color="rgba(0,0,0,0)"),
    hoverinfo="skip", showlegend=False,
    name="95% CI"
))

# 25–75% confidence band
fig.add_trace(go.Scatter(
    x=np.concatenate([years, years[::-1]]),
    y=np.concatenate([gdp_q75, gdp_q25[::-1]]),
    fill="toself", fillcolor=band2,
    line=dict(color="rgba(0,0,0,0)"),
    hoverinfo="skip", showlegend=False,
    name="50% CI"
))

# Median line
fig.add_trace(go.Scatter(
    x=years, y=gdp_med, mode="lines",
    line=dict(color=gdp_color, width=3), name="Median GDP",
    hovertemplate="Year: %{x}<br>GDP: $%{y:.1f}T<extra></extra>"
))

# Add alternative GDP measures as dashed lines (optional)
if 'gdp_activity_based' in gdp_median.columns and not gdp_median['gdp_activity_based'].isna().all():
    gdp_activity_med = gdp_median['gdp_activity_based'].values
    fig.add_trace(go.Scatter(
        x=years, y=gdp_activity_med, mode="lines",
        line=dict(color="#A23B72", width=2, dash="dash"),
        name="Activity-based GDP",
        hovertemplate="Year: %{x}<br>GDP: $%{y:.1f}T<extra></extra>"
    ))

if 'gdp_wages_based' in gdp_median.columns:
    gdp_scaled_med = gdp_median['gdp_wages_based'].values
    fig.add_trace(go.Scatter(
        x=years, y=gdp_scaled_med, mode="lines",
        line=dict(color="#F18F01", width=2, dash="dot"),
        name="Wage-scaled GDP",
        hovertemplate="Year: %{x}<br>GDP: $%{y:.1f}T<extra></extra>"
    ))

fig.update_layout(
    title="Simulated GDP Trajectories with Confidence Bands",
    xaxis_title="Year",
    yaxis_title="GDP (Trillions USD)",
    template="plotly_white",
    width=800,
    height=500,
    legend=dict(
        yanchor="top",
        y=0.99,
        xanchor="left",
        x=0.01
    )
)

fig.show()

"""## Probabilistic Plotting"""

COLOR = {
    "DEV": "#E69F00", "FIN": "#D55E00", "BANK": "#A6761D",
    "GOV": "#56B4E9", "CONS": "#0072B2", "INFRA": "#009E73",
    "LAW": "#CC79A7", "STARTUP": "#7570B3",
}

import numpy as np
import plotly.graph_objects as go

# percentile arrays across runs (shape: steps × N)
median_w = np.percentile(all_w, 50, axis=0)
q05_w    = np.percentile(all_w, 5,  axis=0)
q25_w    = np.percentile(all_w, 25, axis=0)
q75_w    = np.percentile(all_w, 75, axis=0)
q95_w    = np.percentile(all_w, 95, axis=0)

def hex_to_rgba(hex_color: str, alpha: float) -> str:
    hex_color = hex_color.lstrip("#")
    r, g, b = (int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    return f"rgba({r},{g},{b},{alpha})"

fig = go.Figure()
sector_groups = []               # save trace indices for each sector

for i, sector in enumerate(SECTORS):
    base = COLOR.get(sector, "#666666")
    band1 = hex_to_rgba(base, 0.20)   # 5–95 % band
    band2 = hex_to_rgba(base, 0.40)   # 25–75 % band

    start_idx = len(fig.data)

    # 5–95 %
    fig.add_trace(go.Scatter(
        x=np.concatenate([time, time[::-1]]),
        y=np.concatenate([q95_w[:, i], q05_w[::-1, i]]),
        fill="toself", fillcolor=band1,
        line=dict(color="rgba(0,0,0,0)"),
        hoverinfo="skip", showlegend=False
    ))

    # 25–75 %
    fig.add_trace(go.Scatter(
        x=np.concatenate([time, time[::-1]]),
        y=np.concatenate([q75_w[:, i], q25_w[::-1, i]]),
        fill="toself", fillcolor=band2,
        line=dict(color="rgba(0,0,0,0)"),
        hoverinfo="skip", showlegend=False
    ))

    # median
    fig.add_trace(go.Scatter(
        x=time, y=median_w[:, i], mode="lines",
        line=dict(color=base, width=2), name=sector,
        hovertemplate="%{x:.2f}<br>" + sector + ": %{y:.1f} kUSD<extra></extra>"
    ))

    sector_groups.append(range(start_idx, len(fig.data)))  # save indices

# build dropdown buttons after all traces exist
total_traces = len(fig.data)
buttons = [
    dict(label="All",
         method="update",
         args=[{"visible": [True] * total_traces}])
]

for sector, group in zip(SECTORS, sector_groups):
    mask = [idx in group for idx in range(total_traces)]
    buttons.append(dict(
        label=sector,
        method="update",
        args=[{"visible": mask}]
    ))

fig.update_layout(
    title="Simulated Salary Trajectories with Bands",
    xaxis_title="Year",
    yaxis_title="Salary (k USD)",
    updatemenus=[dict(
        type="dropdown",
        direction="up",
        x=1.0, y=0.0,
        xanchor="right", yanchor="bottom",
        buttons=buttons
    )]
)

fig.show()
